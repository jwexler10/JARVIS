#!/usr/bin/env python3
"""
JARVIS Performance Optimization Test Suite
Demonstrates before/after performance improvements.
"""

import time
import asyncio
import sys
import os

# Add current directory to path for imports
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

def test_basic_jarvis():
    """Test basic JARVIS performance"""
    print("üß™ Testing Basic JARVIS Performance")
    print("=" * 50)
    
    try:
        from jarvis import ask_jarvis
        
        test_queries = [
            "Hello JARVIS",
            "How are you today?",
            "What's the weather like?",
            "Thank you"
        ]
        
        times = []
        for query in test_queries:
            start = time.time()
            response = ask_jarvis(query, text_mode=True, speaker="Test")
            duration = (time.time() - start) * 1000
            times.append(duration)
            print(f"   {query}: {duration:.1f}ms -> {response[:50]}...")
        
        avg_time = sum(times) / len(times)
        print(f"\nüìä Basic JARVIS Average: {avg_time:.1f}ms")
        return avg_time
        
    except Exception as e:
        print(f"‚ùå Basic JARVIS test failed: {e}")
        return None

def test_optimized_jarvis():
    """Test optimized JARVIS performance"""
    print("\nüöÄ Testing Optimized JARVIS Performance")
    print("=" * 50)
    
    try:
        from jarvis_optimized import ask_jarvis_optimized, benchmark_jarvis
        
        # Run comprehensive benchmark
        results = benchmark_jarvis(num_tests=4)
        
        if "summary" in results:
            summary = results["summary"]
            print(f"   Average total time: {summary['avg_total_time_ms']:.1f}ms")
            print(f"   Average first token: {summary['avg_first_token_ms']:.1f}ms")
            print(f"   Fastest response: {summary['min_total_time_ms']:.1f}ms")
            print(f"   Success rate: {summary['success_rate']:.1f}%")
            
            return summary['avg_total_time_ms']
        else:
            print("‚ùå Optimized benchmark failed")
            return None
            
    except Exception as e:
        print(f"‚ùå Optimized JARVIS test failed: {e}")
        return None

async def test_streaming_performance():
    """Test streaming performance specifically"""
    print("\n‚ö° Testing Streaming Performance")
    print("=" * 50)
    
    try:
        from jarvis_optimized import ask_jarvis_streaming
        
        query = "Tell me about artificial intelligence in simple terms"
        
        start_time = time.time()
        metrics = await ask_jarvis_streaming(query, "StreamTest")
        
        print(f"   Total response time: {metrics.total_response_time*1000:.1f}ms")
        print(f"   First token time: {metrics.llm_first_token_time*1000:.1f}ms")
        print(f"   LLM completion: {metrics.llm_complete_time*1000:.1f}ms")
        print(f"   TTS completion: {metrics.tts_complete_time*1000:.1f}ms")
        
        return metrics.total_response_time * 1000
        
    except Exception as e:
        print(f"‚ùå Streaming test failed: {e}")
        return None

def main():
    """Run complete performance test suite"""
    print("üéØ JARVIS Performance Optimization Test Suite")
    print("=" * 60)
    
    # Test basic performance
    basic_time = test_basic_jarvis()
    
    # Test optimized performance
    optimized_time = test_optimized_jarvis()
    
    # Test streaming performance
    streaming_time = asyncio.run(test_streaming_performance())
    
    # Performance summary
    print("\nüèÜ PERFORMANCE SUMMARY")
    print("=" * 60)
    
    if basic_time and optimized_time:
        improvement = ((basic_time - optimized_time) / basic_time) * 100
        print(f"üìà Performance Improvement: {improvement:.1f}%")
        print(f"‚è±Ô∏è  Basic JARVIS: {basic_time:.1f}ms")
        print(f"üöÄ Optimized JARVIS: {optimized_time:.1f}ms")
        
        if streaming_time:
            streaming_improvement = ((basic_time - streaming_time) / basic_time) * 100
            print(f"‚ö° Streaming JARVIS: {streaming_time:.1f}ms ({streaming_improvement:.1f}% improvement)")
    
    print("\nüéØ OPTIMIZATION TECHNIQUES USED:")
    print("   ‚úÖ Token streaming (immediate response start)")
    print("   ‚úÖ Fast model selection (gpt-3.5-turbo for simple queries)")
    print("   ‚úÖ Response caching (repeated queries)")
    print("   ‚úÖ Persistent HTTP sessions (connection reuse)")
    print("   ‚úÖ Async TTS processing (overlapped speech synthesis)")
    print("   ‚úÖ Connection pre-warming (reduced cold starts)")
    print("   ‚úÖ Minimal prompt optimization (reduced token overhead)")
    
    print("\nüí° RECOMMENDATIONS:")
    print("   ‚Ä¢ Use --speed flag for maximum performance")
    print("   ‚Ä¢ Deploy in same region as OpenAI (us-east-1)")
    print("   ‚Ä¢ Consider ElevenLabs for fastest TTS")
    print("   ‚Ä¢ Monitor cache hit rate for optimization")

if __name__ == "__main__":
    main()
